{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Máquinas de aprendizaje: Tarea 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felipe Araya Barrera - 201173515-3  \n",
    "Sebastián Vergara Miranda - 201173515-3  \n",
    "9 de septiembre de 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Importaciones necesarias (secciones 1, 2, 3 y 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.io import mmread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> 1. Regresión Lineal Ordinaria (LSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Construcción de dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye un dataframe con los datos provistos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data'\n",
    "df = pd.read_csv(url, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Luego, se realizan los siguientes ajustes sobre este:\n",
    "\n",
    "1. Se elimina columna sin nombre que especifica la posición de cada registro en el dataframe (línea 5). <br> <br>\n",
    "  \n",
    "2. Se almacena por separado la variable *train*, la cual indica si el registro pertenece o no\n",
    "al conjunto de entrenamiento (línea 6). <br> <br>\n",
    "\n",
    "3. Se crea un arreglo que realiza un cambio de notación para los valores que puede tomar\n",
    "la variable *train*, de tal manera que si posee el valor 'T', se cambia por **True**, mientras que si el valor es 'F' se cambia por **False** (línea 7). <br> <br>\n",
    "\n",
    "4. Se crea un arreglo en el que se indica si cada registro pertenece o no al conjunto de\n",
    "prueba (línea 8). <br> <br>\n",
    "\n",
    "5. Se elimina la variable train del dataframe creado (línea 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "istrain_str = df['train']\n",
    "istrain = np.asarray([True if s == 'T' else False for s in istrain_str])\n",
    "istest = np.logical_not(istrain)\n",
    "df = df.drop('train',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Descripción de dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset se compone de 97 registros (pacientes), cada uno de los cuáles está descrito por\n",
    "9 variables, las que se detallan a continuación:\n",
    "\n",
    "1. *lcavol* : Logaritmo del volumen de cáncer presente. <br> <br>\n",
    "    \n",
    "2. *lweight*: Logaritmo del peso de la próstata. <br> <br> \n",
    "\n",
    "3. *age*: Edad. <br> <br>  \n",
    "\n",
    "4. *lbph*: Logaritmo de la cantidad de hiperplasia benigna de próstata. <br> <br> \n",
    "\n",
    "5. *svi*: Indica si existe invasión de la vesícula seminal o no. <br> <br> \n",
    "\n",
    "6. *lcp*: Logaritmo de la penetración capsular. <br> <br>\n",
    "\n",
    "7. *gleason*: Medida del grado de agresividad del cáncer, en base a la escala de Gleason. <br> <br>  \n",
    "\n",
    "8. *pgg45* : Porcentaje que representa la presencia de los patrones de Gleason 4 y 5. <br> <br>  \n",
    "\n",
    "9. *lpsa*: Logaritmo del nivel de antígeno prostático específico (PSA). <br> <br>\n",
    "Además, no existen valores nulos para ningún registro. Más información acerca del dataset (principalmente estadística) puede encontrarse al ejecutar los siguientes comandos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 9 columns):\n",
      "lcavol     97 non-null float64\n",
      "lweight    97 non-null float64\n",
      "age        97 non-null int64\n",
      "lbph       97 non-null float64\n",
      "svi        97 non-null int64\n",
      "lcp        97 non-null float64\n",
      "gleason    97 non-null int64\n",
      "pgg45      97 non-null int64\n",
      "lpsa       97 non-null float64\n",
      "dtypes: float64(5), int64(4)\n",
      "memory usage: 6.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.350010</td>\n",
       "      <td>3.628943</td>\n",
       "      <td>63.865979</td>\n",
       "      <td>0.100356</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>-0.179366</td>\n",
       "      <td>6.752577</td>\n",
       "      <td>24.381443</td>\n",
       "      <td>2.478387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.178625</td>\n",
       "      <td>0.428411</td>\n",
       "      <td>7.445117</td>\n",
       "      <td>1.450807</td>\n",
       "      <td>0.413995</td>\n",
       "      <td>1.398250</td>\n",
       "      <td>0.722134</td>\n",
       "      <td>28.204035</td>\n",
       "      <td>1.154329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.347074</td>\n",
       "      <td>2.374906</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.512824</td>\n",
       "      <td>3.375880</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.731656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.446919</td>\n",
       "      <td>3.623007</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.300105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.798508</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.591516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.127041</td>\n",
       "      <td>3.876396</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.178655</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.056357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.821004</td>\n",
       "      <td>4.780383</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.326302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.904165</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.582932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lcavol    lweight        age       lbph        svi        lcp  \\\n",
       "count  97.000000  97.000000  97.000000  97.000000  97.000000  97.000000   \n",
       "mean    1.350010   3.628943  63.865979   0.100356   0.216495  -0.179366   \n",
       "std     1.178625   0.428411   7.445117   1.450807   0.413995   1.398250   \n",
       "min    -1.347074   2.374906  41.000000  -1.386294   0.000000  -1.386294   \n",
       "25%     0.512824   3.375880  60.000000  -1.386294   0.000000  -1.386294   \n",
       "50%     1.446919   3.623007  65.000000   0.300105   0.000000  -0.798508   \n",
       "75%     2.127041   3.876396  68.000000   1.558145   0.000000   1.178655   \n",
       "max     3.821004   4.780383  79.000000   2.326302   1.000000   2.904165   \n",
       "\n",
       "         gleason       pgg45       lpsa  \n",
       "count  97.000000   97.000000  97.000000  \n",
       "mean    6.752577   24.381443   2.478387  \n",
       "std     0.722134   28.204035   1.154329  \n",
       "min     6.000000    0.000000  -0.430783  \n",
       "25%     6.000000    0.000000   1.731656  \n",
       "50%     7.000000   15.000000   2.591516  \n",
       "75%     7.000000   40.000000   3.056357  \n",
       "max     9.000000  100.000000   5.582932  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Normalización de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de llevar la construcción de un modelo predictivo, es necesario normalizar los datos,\n",
    "pues se está trabajando con variables medidas en unidades y escalas diferentes. Al normalizar,\n",
    "es posible realizar comparaciones razonables entre ellas. Así, cada variable numérica se escala\n",
    "en el rango [0,1]. Para esta instancia en particular, los valores de *lpsa* se mantienen respecto al dataset original. También, se observa que la media de cada variable tiende a 0 y la varianza es muy cercana a 1. Para la normalización, se lleva a cabo el siguiente procedimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df_scaled['lpsa'] = df['lpsa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Construcción de modelo de regresión lineal ordinaria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el modelo, se ejecutan los siguientes pasos: <br> \n",
    "1. Primero, se separa el conjunto de datos en dos partes: La primera contiene solo las\n",
    "variables predictivas ($C_1$), mientras que la segunda, sólo la variable a predecir ($C_2$).<br> <br> \n",
    "2. En $C_1$ , se agrega una nueva variable, que representa el intercepto que será utilizado por\n",
    "cada registro para construir el modelo de regresión. <br> <br>\n",
    "3. Tanto $C_1$ como $C_2$ se subdividen en un conjunto que contiene registros que pertenecen\n",
    "al set de entrenamiento ($C_{11}$ y $C_{21}$) y en otro que agrupa los que pertenecen al set de prueba ($C_{21}$ y $C_{22}$ ). <br> <br>\n",
    "4. Finalmente, para implementar la regresión lineal, se entrega a la función correspondiente\n",
    "los conjuntos $C_{11}$ y $C_{21}$ como argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_scaled.ix[: , :-1]\n",
    "N = X.shape[0]\n",
    "X.insert(X.shape[1], 'intercept', np.ones(N))\n",
    "y = df_scaled['lpsa']\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "linreg = lm.LinearRegression(fit_intercept = False)\n",
    "linreg.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 Pesos y Z-score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los pesos, errores estándar y Z-score para cada variable del\n",
    "modelo elaborado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pesos\n",
    "weights = linreg.coef_\n",
    "#Error estandar\n",
    "SEM = np.asarray(Xtrain.std()) / np.sqrt(len(Xtrain))\n",
    "#A partir de lo anterior, se calculan los Z-score de cada variable\n",
    "Z_score = weights / SEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla 1 resume los valores obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table style=\"width:100%\">\n",
    " <caption>Tabla 1: Peso, SEM y Z-score de cada variable del modelo LSS.</caption>\n",
    "  <tr>\n",
    "    <th>**Variable**</th>\n",
    "    <th>**Peso**</th>\n",
    "    <th>**Error Estándar (SEM)**</th>\n",
    "    <th>**Z-score**</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*lcavol*</td>\n",
    "    <td>0.68</td>\n",
    "    <td>0.13</td>\n",
    "    <td>5.22</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*lweight*</td>\n",
    "    <td>0.26</td>\n",
    "    <td>0.14</td>\n",
    "    <td>1.92</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*age*</td>\n",
    "    <td>-0.14</td>\n",
    "    <td>0.12</td>\n",
    "    <td>-1.14</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*lbph*</td>\n",
    "    <td>0.21</td>\n",
    "    <td>0.12</td>\n",
    "    <td>1.69</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*svi*</td>\n",
    "    <td>0.30</td>\n",
    "    <td>0.12</td>\n",
    "    <td>2.44</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*lcp*</td>\n",
    "    <td>-0.29</td>\n",
    "    <td>0.12</td>\n",
    "    <td>-2.33</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*gleason*</td>\n",
    "    <td>-0.02</td>\n",
    "    <td>0.12</td>\n",
    "    <td>-0.18</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*pgg45*</td>\n",
    "    <td>0.27</td>\n",
    "    <td>0.13</td>\n",
    "    <td>2.08</td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a estos resultados, las variables que presentan una mayor correlación con la\n",
    "variable a predecir son *lcavol*, *svi* y *lcp*. Si se utiliza un nivel de significancia del 5 %, entonces existe evidencia suficiente para afirmar que la variable **NO** presenta una fuerte relación con la respuesta si Z-score ∈ [-1.668, 1.668]. Ante esto, existe evidencia suficiente para afirmar que las variables *age* y *gleason* no están relacionadas con la variable a predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6 Estimación de error de predicción**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente, se calcula el error real del modelo (error cuadrático medio), obteniéndose\n",
    "MSE = 0.52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521274005508\n"
     ]
    }
   ],
   "source": [
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "print mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se evalúan los resultados obtenidos por medio del método cross validation.\n",
    "Se estudian dos casos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso 1**: K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956514631616\n"
     ]
    }
   ],
   "source": [
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "k_fold1 = cross_validation.KFold(len(Xm), 5)\n",
    "mse_cv1 = 0\n",
    "for k, (train, val) in enumerate(k_fold1):\n",
    "  linreg = lm.LinearRegression(fit_intercept = False)\n",
    "  linreg.fit(Xm[train], ym[train])\n",
    "  yhat_val = linreg.predict(Xm[val])\n",
    "  mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "  mse_cv1 += mse_fold\n",
    "mse_cv1 = mse_cv1 / 5\n",
    "print mse_cv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, para K = 5, se obtiene un error $e_1$ = 0.96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso 2**: K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757237472963\n"
     ]
    }
   ],
   "source": [
    "k_fold2 = cross_validation.KFold(len(Xm), 10)\n",
    "mse_cv2 = 0\n",
    "for k, (train, val) in enumerate(k_fold2):\n",
    "  linreg = lm.LinearRegression(fit_intercept = False)\n",
    "  linreg.fit(Xm[train], ym[train])\n",
    "  yhat_val = linreg.predict(Xm[val])\n",
    "  mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "  mse_cv2 += mse_fold\n",
    "mse_cv2 = mse_cv2 / 10\n",
    "print mse_cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, con K = 10, se obtiene un error $e_2$ = 0.76. <br>\n",
    "\n",
    "Se observa que para los casos 1 y 2, el error es considerablemente mayor respecto a MSE.\n",
    "Esto quiere decir que existe un alto nivel de dependencia del modelo respecto a los datos\n",
    "usados para construirlo. En otras palabras, el modelo está sobre ajustado (*overfitting*), por\n",
    "lo que entregará buenas predicciones para los casos pertenecientes al dataset original, pero\n",
    "no será el más apropiado si se desea predecir el valor de la variable de interés para casos de\n",
    "pacientes que no se encuentran dentro del conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7 Error de predicción por dato**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia la validez de suponer que los errores de predicción sobre los datos de entrenamiento siguen una distribución normal. Con la ejecución del siguiente código, se obtiene un gráfico que muestra el comportamiento de los errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Se estima error de prediccion por cada dato de entrenamiento\n",
    "yhat_train = linreg.predict(Xtrain)\n",
    "ytrain_array = np.asarray(ytrain)\n",
    "error = yhat_train - ytrain_array\n",
    "#Se genera grafico de errores\n",
    "stats.probplot(error, dist='norm', plot=plt)\n",
    "plt.title('Siguen los errores de prediccion sobre el conjunto de entrenamiento una distribucion normal?')\n",
    "plt.ylabel('Error dato de entrenamiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el conjunto de errores puede representarse mediante una recta, idea que\n",
    "se reafirma al notar que el coeficiente de correlación es $R^2$ = 0,9913. Como consecuencia de\n",
    "lo anterior, y de acuerdo con el marco teórico, es correcto señalar que los errores siguen una\n",
    "distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Selección de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selección vía FSS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa *Forward Step-wise Selection* (FSS). Para agregar una variable al modelo, se utiliza como criterio el mínimo MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fss(x, y, names_x, k = 10000):\n",
    "  mse_train = []\n",
    "  p = x.shape[1]-1\n",
    "  k = min(p, k)\n",
    "  names_x = np.array(names_x)\n",
    "  remaining = range(0, p)\n",
    "  selected = [p]\n",
    "  current_score = 0.0\n",
    "  best_new_score = 0.0\n",
    "  while [remaining] and len(selected)<=k :\n",
    "    score_candidates = []\n",
    "    for candidate in remaining:\n",
    "      model = lm.LinearRegression(fit_intercept=False)\n",
    "      indexes = selected + [candidate]\n",
    "      x_train = x[:,indexes]\n",
    "      predictions_train = model.fit(x_train, y).predict(x_train)\n",
    "      residuals_train = predictions_train - y\n",
    "      mse_candidate = np.mean(np.power(residuals_train, 2))\n",
    "      score_candidates.append((mse_candidate, candidate))\n",
    "    score_candidates.sort()\n",
    "    score_candidates[:] = score_candidates[::-1]\n",
    "    best_new_score, best_candidate = score_candidates.pop()\n",
    "    remaining.remove(best_candidate)\n",
    "    selected.append(best_candidate)\n",
    "    print \"selected = %s ...\"%(names_x[best_candidate])\n",
    "    print \"total variables = %d, mse = %f\"%(len(indexes),best_new_score)\n",
    "    mse_train.append(best_new_score)\n",
    "  return selected, mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se aplica la función creada sobre los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSS, error de entrenamiento:\n",
      "selected = Lcavol ...\n",
      "total variables = 2, mse = 0.664606\n",
      "selected = Lweight ...\n",
      "total variables = 3, mse = 0.553610\n",
      "selected = Svi ...\n",
      "total variables = 4, mse = 0.521011\n",
      "selected = Lbph ...\n",
      "total variables = 5, mse = 0.489776\n",
      "selected = Pgg45 ...\n",
      "total variables = 6, mse = 0.478648\n",
      "selected = Lcp ...\n",
      "total variables = 7, mse = 0.455818\n",
      "selected = Age ...\n",
      "total variables = 8, mse = 0.439363\n",
      "selected = Gleason ...\n",
      "total variables = 9, mse = 0.439200\n"
     ]
    }
   ],
   "source": [
    "names_regressors = [\"Lcavol\", \"Lweight\", \"Age\", \"Lbph\", \"Svi\", \"Lcp\", \"Gleason\", \"Pgg45\"]\n",
    "print \"FSS, error de entrenamiento:\"\n",
    "seleccionados, mse_train = fss(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al mismo tiempo, se modifica la función anterior para aplicar FSS sobre los datos de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xm_test = Xtest.as_matrix()\n",
    "ym_test = ytest.as_matrix()\n",
    "\n",
    "def fss_test(x,x_t, y, y_t, selected, names_x):\n",
    "  score = []\n",
    "  indexes = []\n",
    "  indexes.append(selected[0])\n",
    "  for i in range(1, len(selected)):\n",
    "    model = lm.LinearRegression(fit_intercept=False)\n",
    "    indexes.append(selected[i])\n",
    "    x_train = x[:,indexes]\n",
    "    x_test = x_t[:,indexes]\n",
    "    prediction_test = model.fit(x_train, y).predict(x_test)\n",
    "    residuals_test = prediction_test - y_t\n",
    "    mse_test = np.mean(np.power(residuals_test,2))\n",
    "    score.append(mse_test)\n",
    "  return score\n",
    "\n",
    "mse_test = fss_test(Xm,Xm_test,ym,ym_test,seleccionados,names_regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se construye un gráfico con el error de entrenamiento y de pruebas en función de la cantidad de variables. Es pertinente señalar que FSS parte incluyendo el intercepto y luego empieza a agregar variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_predictores = range(1, Xm.shape[1])\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.plot(n_predictores, mse_train, 'bo-', label='FSS train')\n",
    "ax.plot(n_predictores, mse_test, 'ro-', label='FSS test')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.xlabel('Cantidad de variables')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Step-wise Selection (FSS)')\n",
    "\n",
    "plt.axis([0, 9, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo al gráfico obtenido anteriormente, se observa que el error de entrenamiento decrece a medida que aumenta la cantidad de variables del modelo. Por otro lado, el error de prueba decrece al principio, pero después aumenta, lo que implica que el modelo está sobreajustado. Además, se puede ver que el error es mínimo cuando existen tres variables en el modelo. Dichas variables son: *Lcavol*, *Lweight* y *Svi*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selección vía BSS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa *Backward Step-wise Selection* (BSS). Para eliminar una variable, se utiliza mínimo MSE como criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bss(x, y, names_x):\n",
    "  mse_train = []\n",
    "  orden_drop = []\n",
    "  names_x = np.array(names_x)\n",
    "  remaining = range(0, x.shape[1])\n",
    "  selected = range(0,x.shape[1])\n",
    "\n",
    "  model = lm.LinearRegression(fit_intercept=False)\n",
    "  predictions_train = model.fit(x, y).predict(x)\n",
    "  residuals_train = predictions_train - y\n",
    "  mse_train.append(np.mean(np.power(residuals_train, 2)))\n",
    "\n",
    "  while (len(selected) != 1):\n",
    "    score_candidates = []\n",
    "    for candidate in remaining:\n",
    "      model = lm.LinearRegression(fit_intercept=False)\n",
    "      selected.remove(candidate)\n",
    "      indexes = selected\n",
    "      x_train = x[:,indexes]\n",
    "      predictions_train = model.fit(x_train, y).predict(x_train)\n",
    "      residuals_train = predictions_train - y\n",
    "      mse_candidate = np.mean(np.power(residuals_train, 2))\n",
    "      score_candidates.append((mse_candidate, candidate))\n",
    "      selected.append(candidate)\n",
    "    score_candidates.sort()\n",
    "    score_candidates[:] = score_candidates[::-1]\n",
    "    worst_new_score, worst_candidate = score_candidates.pop()\n",
    "    remaining.remove(worst_candidate)\n",
    "    selected.remove(worst_candidate)\n",
    "    print \"selected = %s ...\"%(names_x[worst_candidate])\n",
    "    print \"total variables = %d, mse = %f\"%(len(indexes),worst_new_score)\n",
    "    mse_train.append(worst_new_score)\n",
    "    orden_drop.append(worst_candidate)\n",
    "  orden_drop.append(x.shape[1]-1)\n",
    "\n",
    "  return orden_drop, mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se aplica la función creada sobre los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSS, error de entrenamiento:\n",
      "selected = Gleason ...\n",
      "total variables = 8, mse = 0.439363\n",
      "selected = Age ...\n",
      "total variables = 7, mse = 0.455818\n",
      "selected = Lcp ...\n",
      "total variables = 6, mse = 0.478648\n",
      "selected = Pgg45 ...\n",
      "total variables = 5, mse = 0.489776\n",
      "selected = Lbph ...\n",
      "total variables = 4, mse = 0.521011\n",
      "selected = Svi ...\n",
      "total variables = 3, mse = 0.553610\n",
      "selected = Lweight ...\n",
      "total variables = 2, mse = 0.664606\n",
      "selected = Lcavol ...\n",
      "total variables = 1, mse = 1.437036\n"
     ]
    }
   ],
   "source": [
    "names_regressors = [\"Lcavol\", \"Lweight\", \"Age\", \"Lbph\", \"Svi\", \"Lcp\", \"Gleason\", \"Pgg45\", \"intercept\"]\n",
    "print \"BSS, error de entrenamiento:\"\n",
    "seleccionados, mse_train  = bss(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se modifica la función anterior para aplicar BSS sobre los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bss_test(x,x_t, y, y_t, selected, names_x):\n",
    "  score = []\n",
    "  indexes = selected[:]\n",
    "  for i in selected:\n",
    "    model = lm.LinearRegression(fit_intercept=False)\n",
    "    x_train = x[:,indexes]\n",
    "    x_test = x_t[:,indexes]\n",
    "    prediction_test = model.fit(x_train, y).predict(x_test)\n",
    "    residuals_test = prediction_test - y_t\n",
    "    mse_test = np.mean(np.power(residuals_test,2))\n",
    "    score.append(mse_test)\n",
    "    indexes.remove(i)\n",
    "\n",
    "  return score\n",
    "\n",
    "mse_test = bss_test(Xm,Xm_test,ym,ym_test,seleccionados,names_regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se construye un gráfico con el error de entrenamiento y de pruebas de acuerdo al número de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_predictores = range(0, Xm.shape[1])\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.plot(n_predictores, mse_train, 'bo-', label='BSS train')\n",
    "ax.plot(n_predictores, mse_test, 'ro-', label='BSS test')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.xlabel('Cantidad de variables')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Backward Step-wise Selection (BSS)')\n",
    "plt.legend(loc=2)\n",
    "plt.axis([0, 9, 0, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver en el gráfico construído que el error de entrenamiento es menor mientras menos variables tiene el modelo, tal y como debiese ocurrir según la teoría. Por otro lado, el error de prueba decrece al comienzo, pero luego de que la cantidad de variables ha disminuído demasiado, el error aumenta, puesto que no se cuenta con una cantidad apropiada de variables para elaborar un buen modelo. Además, el error mínimo de prueba se obtiene al eliminar cinco variables. Así, se cuenta con un modelo que posee las variables *Lcavol*, *Lweight* y *Svi*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>3.  Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Uso de *Ridge Regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia se procede a implementar un modelo de regresión lineal mediante \"Ridge Regression\" o regresión sesgada. A partir de ello se construirá un gráfico donde\n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "variará entre $10^4$ y $10^{-1}$ y muestra los coeficientes obtenidos como función del parámetro de regularización. Todo esto en base al siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ridge regresion\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pylab as plt\n",
    "#X = X.drop('intercept', axis=1)\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "names_regressors = [\"Lcavol\", \"Lweight\", \"Age\", \"Lbph\", \"Svi\", \"Lcp\", \"Gleason\", \"Pgg45\"]\n",
    "alphas_ = np.logspace(4,-1,base=10)\n",
    "coefs = []\n",
    "model = Ridge(fit_intercept=True,solver=\"svd\")\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    coefs.append(model.coef_)\n",
    "ax = plt.gca()\n",
    "for y_arr, label in zip(np.squeeze(coefs).T, names_regressors):\n",
    "    #print alphas_.shape\n",
    "    #print y_arr.shape\n",
    "    plt.plot(alphas_, y_arr, label=label)\n",
    "plt.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Pesos')\n",
    "plt.title('Regresion sesgada o Ridge')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia para este gráfico que los valores de \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$ \n",
    "son prácticamente nulos entre $10^4$ y $10^{3}$. Por otro lado, las variables con mayor variabilidad, como $Lcavol$ y $Svi$, son beneficiadas porque la penalización de \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "en este caso las afecta menos. Además se puede notar que a medida que aumenta\n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$ las variables son afectadas de manera suave por la penalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Uso de Lasso Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora se hará la regresión lineal con el método Lasso, y de la misma forma que se hizo anteriormente, se construirá un gráfico con los λ ubicados (para este caso) en un intervalo entre $10^1$ y $10^{−2}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasso regresion\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pylab as plt\n",
    "alphas_ = np.logspace(1,-2,base=10)\n",
    "coefs = []\n",
    "model = Lasso(fit_intercept=True)\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    coefs.append(model.coef_)\n",
    "ax = plt.gca()\n",
    "for y_arr, label in zip(np.squeeze(coefs).T, names_regressors):\n",
    "    #print alphas_.shape\n",
    "    #print y_arr.shape\n",
    "    plt.plot(alphas_, y_arr, label=label)\n",
    "plt.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Pesos')\n",
    "plt.title('Regresion Lasso')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se aprecia que la regularización con el método Lasso es mucho más fuerte en comparación que con el uso de la Ridge Regression. Además, la mayor variabilidad la presenta $Lcavol$, que se hace nulo con un \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "igual a 1. Por otro lado se identifica que la variable $Gleason$ tiene un peso nulo durante todo el intervalo de \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$,\n",
    "con lo que se concluye que su importancia a la hora de determinar el cáncer prostático es muy pequeña. En consecuencia es más conveniente utilizar el método Lasso para la regresión lineal, utilizando un parámetro de regulación óptimo para la situación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) y d) Error de regularización con *Ridge Regression* y *Lasso Regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los errores de entrenamiento y de pruebas como función del parámetro de regularización para *Ridge Regression* como para *Lasso Regression* se basa en los siguientes códigos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#error ridge regresion\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "alphas_ = np.logspace(2,-2,base=10)\n",
    "coefs = []\n",
    "model = Ridge(fit_intercept=True)\n",
    "mse_test = []\n",
    "mse_train = []\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    yhat_train = model.predict(Xtrain)\n",
    "    yhat_test = model.predict(Xtest)\n",
    "    mse_train.append(np.mean(np.power(yhat_train - ytrain, 2)))\n",
    "    mse_test.append(np.mean(np.power(yhat_test - ytest, 2)))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas_,mse_train,label='train error ridge')\n",
    "ax.plot(alphas_,mse_test,label='test error ridge')\n",
    "plt.legend(loc=2)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error cuadratico medio')\n",
    "plt.title('Regresion Ridge')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#error lasso regresion\n",
    "alphas_ = np.logspace(0.5,-2,base=10)\n",
    "coefs = []\n",
    "model = Lasso(fit_intercept=True)\n",
    "mse_test = []\n",
    "mse_train = []\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    yhat_train = model.predict(Xtrain)\n",
    "    yhat_test = model.predict(Xtest)\n",
    "    mse_train.append(np.mean(np.power(yhat_train - ytrain, 2)))\n",
    "    mse_test.append(np.mean(np.power(yhat_test - ytest, 2)))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas_,mse_train,label='train error lasso')\n",
    "ax.plot(alphas_,mse_test,label='test error lasso')\n",
    "plt.legend(loc=2)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error cuadratico medio')\n",
    "plt.title('Regresion Lasso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos gráficos se puede identificar que ocurre el efecto de \"sobreajuste\" u *overfitting* del modelo. Cuando los valores de \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$ \n",
    "van disminuyendo y se comienza a depender en mayor medida de las diferentes variables del estudio, va a llegar un punto donde los errores de validación comenzarán a ascender, mientras que los errores con los datos de entrenamiento seguirán descendiendo. Este punto de inflexión es llamado el mínimo error cuadrático medio, que para Ridge se calcula con un \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "cercano a 10 y para Lasso se asocia a un \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "cercano a 0,1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Validación Cruzada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar el valor del parámetro de regularización en los métodos anteriores, se ocupará la validación cruzada, y se trabajará en base a los siguientes algoritmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MSER(y,yhat): return np.mean(np.power(y-yhat,2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "k_fold = cross_validation.KFold(len(Xm),10)\n",
    "best_cv_mse = float(\"inf\")\n",
    "model = Ridge(fit_intercept=True)\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    mse_list_k10 = [MSER(model.fit(Xm[train], ym[train]).predict(Xm[val]), ym[val]) for train, val in k_fold]\n",
    "    if np.mean(mse_list_k10) < best_cv_mse:\n",
    "        best_cv_mse = np.mean(mse_list_k10)\n",
    "        best_alpha = a\n",
    "#print \"mejor parametro lambda ridge=%f, error cuadratico medio(CV)=%f\"%(best_alpha,best_cv_mse)\n",
    "\n",
    "def MSEL(y,yhat): return np.mean(np.power(y-yhat,2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "k_fold = cross_validation.KFold(len(Xm),10)\n",
    "best_cv_mse = float(\"inf\")\n",
    "model = Lasso(fit_intercept=True)\n",
    "for a in alphas_:\n",
    "    model.set_params(alpha=a)\n",
    "    mse_list_k10 = [MSEL(model.fit(Xm[train], ym[train]).predict(Xm[val]), ym[val]) for train, val in k_fold]\n",
    "    if np.mean(mse_list_k10) < best_cv_mse:\n",
    "        best_cv_mse = np.mean(mse_list_k10)\n",
    "        best_alpha = a\n",
    "#print \"mejor parametro lambda lasso=%f, error cuadratico medio(CV)=%f\"%(best_alpha,best_cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores quedan como lo indica la siguiente tabla:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    " <caption>Tabla 2: Estimación del parámetro de regularización usando validación cruzada en Ridge y Lasso</caption>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>***Ridge Regression***</th>\n",
    "    <th>***Lasso Regression***</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$</td>\n",
    "    <td>2.2229</td>\n",
    "    <td>0.0100</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*Error cuadrático medio*</td>\n",
    "    <td>0.7519</td>\n",
    "    <td>0.7587</td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los gráficos construidos para 3.c y 3.d se aprecia que los errores cuatráticos medios rondan los valores entre 0.4 y 0.5, mientras que para la *cross-validation*, los MSE se ubican entre 0.75 y 0.76. Esto puede deberse a que la cantidad de datos usando en esta tarea es muy pequeña, y donde también pudo haber influido el *overfitting* que ocurre en ambos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4.  Predicción de utilidades de películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso se puede identificar que el tamaño de las matrices a trabajar son muy grandes, por lo mismo se usa el comando *csr_matrix* importado de *scipy.sparse* porque además de ser una matriz de con muchos elementos iguales a cero, es muy importante mantener el formato disperso de las entradas, ya que esto permite un gran ahorro de uso de la memoria principal. Además se utilizó el archivo donde están presentes los ganadores del premio **Oscar**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que aumenta la regularización, el valor del coeficiente también aumenta, hasta que llega a un punto en el que\n",
    "tal valor comienza a descender. En ese modo de testeo se buscó calcular el coeficiente de determinación. En primera instancia se usó *Lasso Regression*, según lo que se obtuvo en la pregunta anterior, donde el coeficiente más alto obtenido es de 0.5847 con un \n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$ igual a $10^6$. Luego se decidió utilizar *Ridge Regression* donde el valor más alto obtenido para el coeficiente es de 0.4786 con un $\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$ igual a $10^6$. Para ambos casos este valor de coeficiente se obtuvo considerando el\n",
    "$\n",
    "\\begin{equation} \n",
    "\\lambda\n",
    "\\end{equation}\n",
    "$\n",
    "que lo maximiza, donde ninguno de ellos logra obtener el coeficiente esperado, de 0.75. El código que se muestra a continuación es el correspondiente al que usa la regresión *Lasso*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datos de entrenamiento\n",
    "x = csr_matrix(mmread('train.x.mm'))\n",
    "y = np.loadtxt('train.y.dat')\n",
    "\n",
    "#Se construye modelo, utilizando Lasso\n",
    "maximizador = 10**6 #coeficiente que maximiza el coeficiente de determinacion\n",
    "model = Lasso(fit_intercept=True)\n",
    "model.set_params(alpha=maximizador)\n",
    "model.fit(x, y)\n",
    "\n",
    "#datos de prueba\n",
    "x_test = csr_matrix(mmread('test.x.mm'))\n",
    "y_test = np.loadtxt('test.y.dat')\n",
    "coef = model.score(x_test, y_test)\n",
    "print 'El coeficiente de determinacion es de:', coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
